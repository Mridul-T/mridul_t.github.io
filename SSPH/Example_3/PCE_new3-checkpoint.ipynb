{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pickle\n",
    "from scipy.integrate import quad ## FUNCTIONS TO IMPLEMENT GAUSS-QUADRATURE\n",
    "# from scipy.integrate import quad_vec ## FUNCTIONS TO IMPLEMENT GAUSS-QUADRATURE\n",
    "from scipy.special import hermite,legendre\n",
    "from scipy.linalg import eigh\n",
    "import time\n",
    "np.random.seed(20)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will set the default to float32 or float64 but not float16\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_device = torch.cuda.current_device()\n",
    "# torch.cuda.get_device_properties(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMMRG\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:749: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:433.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "# Set the default tensor type to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "else:\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scalar product of the orthonormal probabilistic Hermite polynomials H_2(x) and H_2(x) is approximately 1.0000000000000022\n",
      "The scalar product of the orthonormal probabilistic Hermite polynomials H_2(x) and H_3(x) is approximately 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMMRG\\AppData\\Local\\Temp\\ipykernel_1516\\293942335.py:3: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n",
      "  return lambda x: hermite(n)(x/np.sqrt(2.0)) / np.sqrt((2.0)**n * np.math.factorial(n))\n"
     ]
    }
   ],
   "source": [
    "# Define the orthonormal probabilistic Hermite polynomial\n",
    "def orthonormal_prob_hermite(n):\n",
    "    return lambda x: hermite(n)(x/np.sqrt(2.0)) / np.sqrt((2.0)**n * np.math.factorial(n))\n",
    "\n",
    "# Define the orthonormal probabilistic Legendre polynomial\n",
    "def orthonormal_legendre(n):\n",
    "    return lambda x: legendre(n)(x) * np.sqrt(n+0.5)\n",
    "\n",
    "# Define the integrand for scalar product\n",
    "def scalar_int(i,j):\n",
    "    return lambda x:orthonormal_prob_hermite(i)(x) * orthonormal_prob_hermite(j)(x) * np.exp(-(x**2/2))/np.sqrt(2*np.pi)\n",
    "\n",
    "# Define the integrand for triple product\n",
    "def triple_int(i,j,k):\n",
    "    return lambda x:orthonormal_prob_hermite(i)(x) * orthonormal_prob_hermite(j)(x)* orthonormal_prob_hermite(k)(x) * np.exp(-(x**2/2))/np.sqrt(2*np.pi)\n",
    "\n",
    "# Integrate to verify orthonormality\n",
    "integral1, _ = quad(scalar_int(2,2), -np.inf, np.inf)\n",
    "integral2, _ = quad(scalar_int(2,3), -np.inf, np.inf)\n",
    "print(f\"The scalar product of the orthonormal probabilistic Hermite polynomials H_2(x) and H_2(x) is approximately {integral1}\")\n",
    "print(f\"The scalar product of the orthonormal probabilistic Hermite polynomials H_2(x) and H_3(x) is approximately {integral2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=1\n",
    "con = 1\n",
    "particles_per_unit=50*k\n",
    "dx = con/particles_per_unit\n",
    "ratio=1.6\n",
    "h=ratio*dx\n",
    "c_ = 2 * h\n",
    "q=5\n",
    "max_ord=5\n",
    "n_samples=96 * 50\n",
    "\n",
    "J = particles_per_unit**2 #total number of particles\n",
    "rho = 1000*torch.ones(J)  ## for steel\n",
    "mass = rho * dx**2\n",
    "rho0=rho\n",
    "# vis=0.05\n",
    "T = 0.3               # Total time of integration\n",
    "dt = 0.001          # Time step\n",
    "N = int(T/dt)\n",
    "N  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_combinations(q, max_ord,current_sum=0, current_combination=None, all_combinations=None):\n",
    "    if current_combination is None:\n",
    "        current_combination = []\n",
    "    if all_combinations is None:\n",
    "        all_combinations = []\n",
    "        \n",
    "    if len(current_combination) == q:\n",
    "        if current_sum < max_ord:\n",
    "            all_combinations.append(current_combination[:])\n",
    "        return all_combinations\n",
    "    \n",
    "    for i in range(max_ord - current_sum):\n",
    "        current_combination.append(i)\n",
    "        find_combinations(q, max_ord,current_sum + i, current_combination, all_combinations)\n",
    "        current_combination.pop()\n",
    "\n",
    "    return all_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=np.array(find_combinations(q,max_ord))\n",
    "P=index.shape[0]\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMMRG\\anaconda3\\Lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "x_values = torch.linspace(0, 1*con, particles_per_unit)\n",
    "y_values = torch.linspace(0, 1*con, particles_per_unit)\n",
    "# Create a meshgrid\n",
    "X, Y = torch.meshgrid(x_values, y_values)\n",
    "# print(X.shape)\n",
    "\n",
    "# Flatten the meshgrid arrays for 1D function application\n",
    "X_flat = X.flatten()\n",
    "Y_flat = Y.flatten()\n",
    "\n",
    "temp=tuple(zip(X_flat,Y_flat))\n",
    "temp=torch.tensor(temp)\n",
    "coords=torch.zeros(P,J,2)\n",
    "coords[0]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the squared exponential kernel for 2D\n",
    "def squared_exponential_kernel_2d(pos1, pos2, sigma=1, l=0.1):\n",
    "    # Compute pairwise squared distances\n",
    "    dists_sq = np.sum((pos1[:, None, :] - pos2[None, :, :])**2, axis=2)\n",
    "    # Compute the kernel\n",
    "    K = sigma * np.exp(-0.5 * dists_sq / l**2)\n",
    "    return K\n",
    "\n",
    "# Define the mean function\n",
    "def mean(x):\n",
    "    return 0.05\n",
    "\n",
    "# Flatten the meshgrid into a single array of 2D positions\n",
    "positions = np.column_stack((X.ravel(), Y.ravel()))\n",
    "\n",
    "# Construct the covariance kernel for the 2D domain\n",
    "sigma = 0.001\n",
    "l = 0.01\n",
    "K = squared_exponential_kernel_2d(positions, positions, sigma=sigma, l=l)\n",
    "\n",
    "# Compute the eigenvalues and eigenvectors of the covariance matrix\n",
    "eigenvalues, eigenvectors = eigh(K)\n",
    "\n",
    "# Sort the eigenvalues and eigenvectors in descending order\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = torch.tensor(eigenvalues[idx])\n",
    "eigenvectors = torch.tensor(eigenvectors[:, idx])\n",
    "\n",
    "# Define the coefficient function u\n",
    "def vis(q):\n",
    "    if(q==0):\n",
    "        f = mean(positions)\n",
    "    else:\n",
    "        f = eigenvectors[:, :q] * np.sqrt(eigenvalues[:q])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_init():\n",
    "    temp = torch.zeros((2,particles_per_unit,particles_per_unit))\n",
    "    \n",
    "    temp[0] = 0.25 * torch.sin(2*np.pi*X) * torch.sin(2*np.pi*Y)\n",
    "    temp[1] = -0.1 * torch.sin(2*np.pi*X) * torch.sin(2*np.pi*Y)\n",
    "    \n",
    "    u0 = torch.zeros((2, J))\n",
    "    u0[0,:]=temp[0,:,:].flatten()\n",
    "    u0[1,:]=temp[1,:,:].flatten()\n",
    "    return u0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=create_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMMRG\\AppData\\Local\\Temp\\ipykernel_1516\\1631368004.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ut[:,0,:,0]=torch.tensor(values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.1088e-08,\n",
       "         -5.5897e-09,  7.6427e-15],\n",
       "        [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  4.4350e-09,\n",
       "          2.2359e-09, -3.0571e-15]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut = torch.zeros((2, P, J, N+1))\n",
    "ut[:,0,:,0]=torch.tensor(values)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMMRG\\AppData\\Local\\Temp\\ipykernel_1516\\293942335.py:3: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n",
      "  return lambda x: hermite(n)(x/np.sqrt(2.0)) / np.sqrt((2.0)**n * np.math.factorial(n))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_b=torch.zeros((max_ord,max_ord),device='cpu')\n",
    "for i in range(max_ord):\n",
    "    for j in range(max_ord):\n",
    "        pre_b[i,j],_=quad(lambda x: orthonormal_prob_hermite(i)(x)*orthonormal_prob_hermite(j)(x)* np.exp(-(x**2/2))/np.sqrt(2*np.pi), -np.inf, np.inf)\n",
    "        if(abs(pre_b[i,j])<1e-10):\n",
    "            pre_b[i,j]=0\n",
    "pre_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMMRG\\AppData\\Local\\Temp\\ipykernel_1516\\293942335.py:3: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n",
      "  return lambda x: hermite(n)(x/np.sqrt(2.0)) / np.sqrt((2.0)**n * np.math.factorial(n))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000]],\n",
       "\n",
       "        [[ 0.0000,  1.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0000,  0.0000,  1.4142,  0.0000,  0.0000],\n",
       "         [ 0.0000,  1.4142,  0.0000,  1.7321,  0.0000],\n",
       "         [ 0.0000,  0.0000,  1.7321,  0.0000,  2.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  2.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  1.4142,  0.0000,  1.7321,  0.0000],\n",
       "         [ 1.0000,  0.0000,  2.8284,  0.0000,  2.4495],\n",
       "         [ 0.0000,  1.7321,  0.0000,  4.2426,  0.0000],\n",
       "         [ 0.0000,  0.0000,  2.4495,  0.0000,  5.6569]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  1.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  1.7321,  0.0000,  2.0000],\n",
       "         [ 0.0000,  1.7321,  0.0000,  4.2426,  0.0000],\n",
       "         [ 1.0000,  0.0000,  4.2426,  0.0000,  7.3485],\n",
       "         [ 0.0000,  2.0000,  0.0000,  7.3485,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  2.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  2.4495,  0.0000,  5.6569],\n",
       "         [ 0.0000,  2.0000,  0.0000,  7.3485,  0.0000],\n",
       "         [ 1.0000,  0.0000,  5.6569,  0.0000, 14.6969]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_c=torch.zeros((max_ord,max_ord,max_ord),device='cpu')\n",
    "for i in range(max_ord):\n",
    "    for j in range(max_ord):\n",
    "        for k in range(max_ord):\n",
    "            pre_c[i,j,k],_=quad(lambda x: orthonormal_prob_hermite(i)(x)*orthonormal_prob_hermite(j)(x)* orthonormal_prob_hermite(k)(x) *np.exp(-(x**2/2))/np.sqrt(2*np.pi), -np.inf, np.inf)\n",
    "            if(abs(pre_c[i,j,k])<1e-10):\n",
    "                pre_c[i,j,k]=0\n",
    "pre_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMMRG\\AppData\\Local\\Temp\\ipykernel_1516\\293942335.py:3: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n",
      "  return lambda x: hermite(n)(x/np.sqrt(2.0)) / np.sqrt((2.0)**n * np.math.factorial(n))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 0.0000, 1.4142, 0.0000, 0.0000],\n",
       "        [0.0000, 1.4142, 0.0000, 1.7321, 0.0000],\n",
       "        [0.0000, 0.0000, 1.7321, 0.0000, 2.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 2.0000, 0.0000]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_d=torch.zeros((max_ord,max_ord),device='cpu')\n",
    "for i in range(max_ord):\n",
    "    for j in range(max_ord):\n",
    "        pre_d[i,j],_=quad(lambda x: x * orthonormal_prob_hermite(i)(x)*orthonormal_prob_hermite(j)(x)* np.exp(-(x**2/2))/np.sqrt(2*np.pi), -np.inf, np.inf)\n",
    "        if(abs(pre_d[i,j])<1e-10):\n",
    "            pre_d[i,j]=0\n",
    "pre_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_b(i, pre_b, index):\n",
    "    print(f\"Starting {i}th iteration\")\n",
    "    b = torch.zeros(size=(P,),device='cpu')\n",
    "    for j in range(P):\n",
    "        b[j]=1\n",
    "        for k in range(q):\n",
    "            b[j]*=vis(k)*pre_b[index[i][k],index[j][k]]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_d(i, pre_d, index):\n",
    "    print(f\"Starting {i}th iteration\")\n",
    "    b = torch.zeros(size=(P,),device='cpu')\n",
    "    for j in range(P):\n",
    "        b[j]=1\n",
    "        for k in range(q):\n",
    "            b[j]*=pre_d[index[i][k],index[j][k]]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMMRG\\AppData\\Local\\Temp\\ipykernel_1516\\495472464.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  index=torch.tensor(index,device='cpu')\n"
     ]
    }
   ],
   "source": [
    "index=torch.tensor(index,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_c(i, pre_c, index):\n",
    "    print(f\"Starting {i}th iteration\")\n",
    "    P, q = index.shape\n",
    "#     index=torch.tensor(index)\n",
    "    # Initialize the output tensor c with ones\n",
    "    c = torch.ones((P, P), dtype=pre_c.dtype, device=pre_c.device)\n",
    "    \n",
    "    # Iterate over the range Q and perform element-wise multiplication for all combinations\n",
    "    for l in range(q):\n",
    "        idx_i = index[i, l]\n",
    "        idx_j = index[:, l].unsqueeze(1).expand(P, P)\n",
    "        idx_k = index[:, l].unsqueeze(0).expand(P, P)\n",
    "        c *= pre_c[idx_i, idx_j, idx_k]\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cores=multiprocessing.cpu_count()\n",
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "\n",
    "b = Parallel(n_jobs=num_cores)(delayed(cal_b)(i, pre_b, index) for i in range(P))\n",
    "c = Parallel(n_jobs=num_cores)(delayed(cal_c)(i, pre_c, index) for i in range(P))\n",
    "d = Parallel(n_jobs=num_cores)(delayed(cal_d)(i, pre_d, index) for i in range(P))\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken 0.06519229809443156 mins\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time taken {(end-start)/60} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([126, 126])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=torch.stack(b,dim=0)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([126, 126])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=torch.stack(d,dim=0)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([126, 126, 126])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=torch.stack(c,dim=0)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_nlist(coords,side_length=float(con)):\n",
    "    # Compute pairwise distance\n",
    "    # Shifts\n",
    "    eps = 0.1*h\n",
    "    shifts = [\n",
    "        (0, 0)\n",
    "    ]\n",
    "    n_particles=coords.size()[0]\n",
    "    # Expand the coordinates\n",
    "    \n",
    "    all_coords = []\n",
    "    for shift in shifts:\n",
    "        shifted_coords = coords + torch.tensor(shift,dtype=coords.dtype)\n",
    "        all_coords.append(shifted_coords)\n",
    "\n",
    "    \n",
    "    all_coords = torch.cat(all_coords, dim=0)\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    dX = all_coords[:, 0][:, None] - coords[:, 0][None, :]\n",
    "    dY = all_coords[:, 1][:, None] - coords[:, 1][None, :]\n",
    "    # print(dX.shape)\n",
    "    # half_domain = side_length / 2\n",
    "    # dX = (dX + half_domain) % side_length - half_domain\n",
    "    # dY = (dY + half_domain) % side_length - half_domain\n",
    "    \n",
    "    distances = torch.sqrt(dX**2 + dY**2)\n",
    "\n",
    "    # Create the neighbor list using a mask for distances < c and excluding self-distances\n",
    "    neighbor_mask = (distances <= c_) & (distances > 0)\n",
    "    # print(neighbor_mask.shape)\n",
    "    # Compute the neighbor list\n",
    "    n_list = [[torch.nonzero(neighbor_mask[i]).squeeze() % n_particles,(dX[i][neighbor_mask[i]]),(dY[i][neighbor_mask[i]])] for i in range(J)]\n",
    "    \n",
    "    return n_list\n",
    "\n",
    "n_list=compute_nlist(coords[0])\n",
    "len(n_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   2,   3,  50,  51,  52, 100, 101, 102, 150])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_CubicSpline(x, y, h, domain_size=1):\n",
    "    alpha = 15/(7*np.pi*h**2)\n",
    "\n",
    "\n",
    "    r = torch.sqrt(x**2 + y**2)\n",
    "    q = r / h\n",
    "    mask1 = q <= 1\n",
    "    mask2 = (q > 1) & (q <= 2)\n",
    "    dz_dx = torch.zeros_like(q)\n",
    "    dz_dy = torch.zeros_like(q)\n",
    "\n",
    "    dz_dx[mask1] = alpha * (-2*q[mask1] + 3/2*q[mask1]**2) * x[mask1] / (r[mask1] * h)\n",
    "    dz_dy[mask1] = alpha * (-2*q[mask1] + 3/2*q[mask1]**2) * y[mask1] / (r[mask1] * h)\n",
    "    dz_dx[mask2] = alpha * (-1/2*(2-q[mask2])**2) * x[mask2] / (r[mask2] * h)\n",
    "    dz_dy[mask2] = alpha * (-1/2*(2-q[mask2])**2) * y[mask2] / (r[mask2] * h)\n",
    "    \n",
    "    return dz_dx, dz_dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_A(x, y, h, m_j, rho_j, epsilon=1e-8):\n",
    "    alpha = 15 / (7 * np.pi * h ** 2)\n",
    "    r = torch.sqrt(x**2 + y**2)\n",
    "    q = r / h\n",
    "    mask1 = q <= 1\n",
    "    mask2 = (q > 1) & (q <= 2)\n",
    "    \n",
    "    W_xx = torch.zeros_like(q)\n",
    "    W_yy = torch.zeros_like(q)\n",
    "    \n",
    "    W_xx[mask1] = alpha * (-2 * q[mask1] + 3/2 * q[mask1]**2) * (x[mask1]) / (r[mask1] * h)\n",
    "    W_yy[mask1] = alpha * (-2 * q[mask1] + 3/2 * q[mask1]**2) * (y[mask1]) / (r[mask1] * h)\n",
    "    W_xx[mask2] = alpha * (-1/2 * (2 - q[mask2])**2) * (x[mask2]) / (r[mask2] * h)\n",
    "    W_yy[mask2] = alpha * (-1/2 * (2 - q[mask2])**2) * (y[mask2]) / (r[mask2] * h)\n",
    "    \n",
    "    A_xx = -torch.sum(m_j * x / rho_j * W_xx)\n",
    "    A_xy = -torch.sum(m_j * x / rho_j * W_yy)\n",
    "    A_yx = -torch.sum(m_j * y / rho_j * W_xx)\n",
    "    A_yy = -torch.sum(m_j * y / rho_j * W_yy)\n",
    "    \n",
    "    A_inv = torch.linalg.inv(torch.tensor([[A_xx, A_xy], [A_yx, A_yy]]))\n",
    "    return A_inv\n",
    "\n",
    "def d_CubicSpline_corrected(x, y, h, m_j, rho_j):\n",
    "    # Compute the gradient of the kernel\n",
    "    dz_dx, dz_dy = d_CubicSpline(x, y, h)\n",
    "    \n",
    "    # Compute the correction matrix A_inv\n",
    "    B = compute_A(x, y, h, m_j, rho_j)\n",
    "    \n",
    "    # Apply the gradient correction\n",
    "    dz_dx_corrected = B[0, 0] * dz_dx + B[0, 1] * dz_dy\n",
    "    dz_dy_corrected = B[1, 0] * dz_dx + B[1, 1] * dz_dy\n",
    "    \n",
    "    return dz_dx_corrected, dz_dy_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPH(c,h,comp,n_list=n_list):\n",
    "    cd=torch.zeros(size=(2,P,c.shape[1]))\n",
    "    for i,data in enumerate(n_list):\n",
    "        neighbors = data[0]\n",
    "        c_i = c[:,i][:,np.newaxis]  # Broadcasting to create the necessary shape\n",
    "        c_i = torch.tensor(np.repeat(c_i.cpu(), len(neighbors), axis=1),device=cd.device)\n",
    "        c_j = c[:,neighbors]\n",
    "        rho_neighbors = rho[neighbors]\n",
    "        mass_neighbors = mass[neighbors]\n",
    "        \n",
    "        c_diff = c_i - c_j\n",
    "        # dx,dy=d_CubicSpline_corrected(data[1],data[2], h, mass_neighbors, rho_neighbors)\n",
    "        dx,dy=d_CubicSpline(data[1],data[2],h)\n",
    "        cd[0,:,i] = torch.einsum(\"N,PN->P\", mass_neighbors / rho_neighbors * dx, c_diff)\n",
    "        cd[1,:,i] = torch.einsum(\"N,PN->P\", mass_neighbors / rho_neighbors * dy, c_diff)\n",
    "    return cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPH_d(c,h,n_list=n_list):\n",
    "    cd=torch.zeros_like(c)\n",
    "    for i,data in enumerate(n_list):\n",
    "        neighbors = data[0]\n",
    "        c_i = c[:,:,i][:,:,np.newaxis]  # Broadcasting to create the necessary shape\n",
    "        c_i = torch.tensor(np.repeat(c_i.cpu(), len(neighbors), axis=2),device=cd.device)\n",
    "        c_j = c[:,:,neighbors]\n",
    "        rho_neighbors = rho[neighbors]\n",
    "        mass_neighbors = mass[neighbors]\n",
    "        c_diff = (c_i - c_j)\n",
    "        # dx,dy=d_CubicSpline_corrected(data[1],data[2], h, mass_neighbors, rho_neighbors)\n",
    "        dx,dy=d_CubicSpline(data[1],data[2],h)\n",
    "        cd[0,:,i] = torch.einsum(\"N,PN->P\",mass_neighbors / rho_neighbors * dx, c_diff[0,:,:])\n",
    "        cd[1,:,i] = torch.einsum(\"N,PN->P\",mass_neighbors / rho_neighbors * dy, c_diff[1,:,:])\n",
    "    return cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xSPH(c,h,n_list=n_list):\n",
    "    cd=torch.zeros_like(c)\n",
    "    for i,data in enumerate(n_list):\n",
    "        neighbors = data[0]\n",
    "        c_i = c[:,:,i][:,:,np.newaxis]  # Broadcasting to create the necessary shape\n",
    "        c_i = torch.tensor(np.repeat(c_i.cpu(), len(neighbors), axis=2),device=cd.device)\n",
    "        c_j = c[:,:,neighbors]\n",
    "        rho_neighbors = rho[neighbors]\n",
    "        mass_neighbors = mass[neighbors]\n",
    "        c_diff = (c_j - c_i)\n",
    "        # dx,dy=d_CubicSpline_corrected(data[1],data[2], h,mass_neighbors,rho_neighbors)\n",
    "        dx,dy=d_CubicSpline(data[1],data[2],h)\n",
    "        # cd[0,:,i] = torch.sum(mass_neighbors / rho_neighbors * c_diff * dx)\n",
    "        cd[0,:,i] = 0.5*torch.einsum(\"N,PN->P\",mass_neighbors / rho_neighbors * dx, c_diff[0,:,:])\n",
    "        cd[1,:,i] = 0.5*torch.einsum(\"N,PN->P\",mass_neighbors / rho_neighbors * dy, c_diff[1,:,:])\n",
    "        # cd[0,0,i] += torch.sum(mass_neighbors / rho_neighbors * (pi_ij) * dx)\n",
    "        # cd[1,0,i] += torch.sum(mass_neighbors / rho_neighbors * (pi_ij) * dy)\n",
    "    return cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to detect and correct outliers based on the median\n",
    "def median_outlier_correction(ut, neighborlist):\n",
    "    # Iterate over all particles\n",
    "    for j in range(ut.shape[2]):\n",
    "        neighbors = neighborlist[j][0]\n",
    "        if len(neighbors) > 0:  # Check if the particle has neighbors\n",
    "            # Compute the median velocity from the neighbors\n",
    "            median_velocity_x = torch.median(ut[0, :, neighbors], dim=1).values\n",
    "            median_velocity_y = torch.median(ut[1, :, neighbors], dim=1).values\n",
    "            # Update the velocity of the current particle to the median velocity of neighbors\n",
    "            ut[0, :, j] = median_velocity_x\n",
    "            ut[1, :, j] = median_velocity_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates=torch.zeros((P,J,2,N+1))\n",
    "coordinates[:,:,:,0]=coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords[1,:51,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mask = torch.logical_or(coords[0,:,0] == 0, coords[0,:,0] == 1)\n",
    "y_mask = torch.logical_or(coords[0,:,1] == 0, coords[0,:,1] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mask[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 126, 2500, 301])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMMRG\\AppData\\Local\\Temp\\ipykernel_1516\\4027079884.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c_i = torch.tensor(np.repeat(c_i.cpu(), len(neighbors), axis=1),device=cd.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed the 1^th timestep\n",
      "Completed the 2^th timestep\n",
      "Completed the 3^th timestep\n",
      "Completed the 4^th timestep\n",
      "Completed the 5^th timestep\n",
      "Completed the 6^th timestep\n",
      "Completed the 7^th timestep\n",
      "Completed the 8^th timestep\n",
      "Completed the 9^th timestep\n",
      "Completed the 10^th timestep\n",
      "Completed the 11^th timestep\n",
      "Completed the 12^th timestep\n",
      "Completed the 13^th timestep\n",
      "Completed the 14^th timestep\n",
      "Completed the 15^th timestep\n",
      "Completed the 16^th timestep\n",
      "Completed the 17^th timestep\n",
      "Completed the 18^th timestep\n",
      "Completed the 19^th timestep\n",
      "Completed the 20^th timestep\n",
      "Completed the 21^th timestep\n",
      "Completed the 22^th timestep\n",
      "Completed the 23^th timestep\n",
      "Completed the 24^th timestep\n",
      "Completed the 25^th timestep\n",
      "Completed the 26^th timestep\n",
      "Completed the 27^th timestep\n",
      "Completed the 28^th timestep\n",
      "Completed the 29^th timestep\n",
      "Completed the 30^th timestep\n",
      "Completed the 31^th timestep\n",
      "Completed the 32^th timestep\n",
      "Completed the 33^th timestep\n",
      "Completed the 34^th timestep\n",
      "Completed the 35^th timestep\n",
      "Completed the 36^th timestep\n",
      "Completed the 37^th timestep\n",
      "Completed the 38^th timestep\n",
      "Completed the 39^th timestep\n",
      "Completed the 40^th timestep\n",
      "Completed the 41^th timestep\n",
      "Completed the 42^th timestep\n",
      "Completed the 43^th timestep\n",
      "Completed the 44^th timestep\n",
      "Completed the 45^th timestep\n",
      "Completed the 46^th timestep\n",
      "Completed the 47^th timestep\n",
      "Completed the 48^th timestep\n",
      "Completed the 49^th timestep\n",
      "Completed the 50^th timestep\n",
      "Completed the 51^th timestep\n",
      "Completed the 52^th timestep\n",
      "Completed the 53^th timestep\n",
      "Completed the 54^th timestep\n",
      "Completed the 55^th timestep\n",
      "Completed the 56^th timestep\n",
      "Completed the 57^th timestep\n",
      "Completed the 58^th timestep\n",
      "Completed the 59^th timestep\n",
      "Completed the 60^th timestep\n",
      "Completed the 61^th timestep\n",
      "Completed the 62^th timestep\n",
      "Completed the 63^th timestep\n",
      "Completed the 64^th timestep\n",
      "Completed the 65^th timestep\n",
      "Completed the 66^th timestep\n",
      "Completed the 67^th timestep\n",
      "Completed the 68^th timestep\n",
      "Completed the 69^th timestep\n",
      "Completed the 70^th timestep\n",
      "Completed the 71^th timestep\n",
      "Completed the 72^th timestep\n",
      "Completed the 73^th timestep\n",
      "Completed the 74^th timestep\n",
      "Completed the 75^th timestep\n",
      "Completed the 76^th timestep\n",
      "Completed the 77^th timestep\n",
      "Completed the 78^th timestep\n",
      "Completed the 79^th timestep\n",
      "Completed the 80^th timestep\n",
      "Completed the 81^th timestep\n",
      "Completed the 82^th timestep\n",
      "Completed the 83^th timestep\n",
      "Completed the 84^th timestep\n",
      "Completed the 85^th timestep\n",
      "Completed the 86^th timestep\n",
      "Completed the 87^th timestep\n",
      "Completed the 88^th timestep\n",
      "Completed the 89^th timestep\n",
      "Completed the 90^th timestep\n",
      "Completed the 91^th timestep\n",
      "Completed the 92^th timestep\n",
      "Completed the 93^th timestep\n",
      "Completed the 94^th timestep\n",
      "Completed the 95^th timestep\n",
      "Completed the 96^th timestep\n",
      "Completed the 97^th timestep\n",
      "Completed the 98^th timestep\n",
      "Completed the 99^th timestep\n",
      "Completed the 100^th timestep\n",
      "Completed the 101^th timestep\n",
      "Completed the 102^th timestep\n",
      "Completed the 103^th timestep\n",
      "Completed the 104^th timestep\n",
      "Completed the 105^th timestep\n",
      "Completed the 106^th timestep\n",
      "Completed the 107^th timestep\n",
      "Completed the 108^th timestep\n",
      "Completed the 109^th timestep\n",
      "Completed the 110^th timestep\n",
      "Completed the 111^th timestep\n",
      "Completed the 112^th timestep\n",
      "Completed the 113^th timestep\n",
      "Completed the 114^th timestep\n",
      "Completed the 115^th timestep\n",
      "Completed the 116^th timestep\n",
      "Completed the 117^th timestep\n",
      "Completed the 118^th timestep\n",
      "Completed the 119^th timestep\n",
      "Completed the 120^th timestep\n",
      "Completed the 121^th timestep\n",
      "Completed the 122^th timestep\n",
      "Completed the 123^th timestep\n",
      "Completed the 124^th timestep\n",
      "Completed the 125^th timestep\n",
      "Completed the 126^th timestep\n",
      "Completed the 127^th timestep\n",
      "Completed the 128^th timestep\n",
      "Completed the 129^th timestep\n",
      "Completed the 130^th timestep\n",
      "Completed the 131^th timestep\n",
      "Completed the 132^th timestep\n",
      "Completed the 133^th timestep\n",
      "Completed the 134^th timestep\n",
      "Completed the 135^th timestep\n",
      "Completed the 136^th timestep\n",
      "Completed the 137^th timestep\n",
      "Completed the 138^th timestep\n",
      "Completed the 139^th timestep\n",
      "Completed the 140^th timestep\n",
      "Completed the 141^th timestep\n",
      "Completed the 142^th timestep\n",
      "Completed the 143^th timestep\n",
      "Completed the 144^th timestep\n",
      "Completed the 145^th timestep\n",
      "Completed the 146^th timestep\n",
      "Completed the 147^th timestep\n",
      "Completed the 148^th timestep\n",
      "Completed the 149^th timestep\n",
      "Completed the 150^th timestep\n",
      "Completed the 151^th timestep\n",
      "Completed the 152^th timestep\n",
      "Completed the 153^th timestep\n",
      "Completed the 154^th timestep\n",
      "Completed the 155^th timestep\n",
      "Completed the 156^th timestep\n",
      "Completed the 157^th timestep\n",
      "Completed the 158^th timestep\n",
      "Completed the 159^th timestep\n",
      "Completed the 160^th timestep\n",
      "Completed the 161^th timestep\n",
      "Completed the 162^th timestep\n",
      "Completed the 163^th timestep\n",
      "Completed the 164^th timestep\n",
      "Completed the 165^th timestep\n",
      "Completed the 166^th timestep\n",
      "Completed the 167^th timestep\n",
      "Completed the 168^th timestep\n",
      "Completed the 169^th timestep\n",
      "Completed the 170^th timestep\n",
      "Completed the 171^th timestep\n",
      "Completed the 172^th timestep\n",
      "Completed the 173^th timestep\n",
      "Completed the 174^th timestep\n",
      "Completed the 175^th timestep\n",
      "Completed the 176^th timestep\n",
      "Completed the 177^th timestep\n",
      "Completed the 178^th timestep\n",
      "Completed the 179^th timestep\n",
      "Completed the 180^th timestep\n",
      "Completed the 181^th timestep\n",
      "Completed the 182^th timestep\n",
      "Completed the 183^th timestep\n",
      "Completed the 184^th timestep\n",
      "Completed the 185^th timestep\n",
      "Completed the 186^th timestep\n",
      "Completed the 187^th timestep\n",
      "Completed the 188^th timestep\n",
      "Completed the 189^th timestep\n",
      "Completed the 190^th timestep\n",
      "Completed the 191^th timestep\n",
      "Completed the 192^th timestep\n",
      "Completed the 193^th timestep\n",
      "Completed the 194^th timestep\n",
      "Completed the 195^th timestep\n",
      "Completed the 196^th timestep\n",
      "Completed the 197^th timestep\n",
      "Completed the 198^th timestep\n",
      "Completed the 199^th timestep\n",
      "Completed the 200^th timestep\n",
      "Completed the 201^th timestep\n",
      "Completed the 202^th timestep\n",
      "Completed the 203^th timestep\n",
      "Completed the 204^th timestep\n",
      "Completed the 205^th timestep\n",
      "Completed the 206^th timestep\n",
      "Completed the 207^th timestep\n",
      "Completed the 208^th timestep\n",
      "Completed the 209^th timestep\n",
      "Completed the 210^th timestep\n",
      "Completed the 211^th timestep\n",
      "Completed the 212^th timestep\n",
      "Completed the 213^th timestep\n",
      "Completed the 214^th timestep\n",
      "Completed the 215^th timestep\n",
      "Completed the 216^th timestep\n",
      "Completed the 217^th timestep\n",
      "Completed the 218^th timestep\n",
      "Completed the 219^th timestep\n",
      "Completed the 220^th timestep\n",
      "Completed the 221^th timestep\n",
      "Completed the 222^th timestep\n",
      "Completed the 223^th timestep\n",
      "Completed the 224^th timestep\n",
      "Completed the 225^th timestep\n",
      "Completed the 226^th timestep\n",
      "Completed the 227^th timestep\n",
      "Completed the 228^th timestep\n",
      "Completed the 229^th timestep\n",
      "Completed the 230^th timestep\n",
      "Completed the 231^th timestep\n",
      "Completed the 232^th timestep\n",
      "Completed the 233^th timestep\n",
      "Completed the 234^th timestep\n",
      "Completed the 235^th timestep\n",
      "Completed the 236^th timestep\n",
      "Completed the 237^th timestep\n",
      "Completed the 238^th timestep\n",
      "Completed the 239^th timestep\n",
      "Completed the 240^th timestep\n",
      "Completed the 241^th timestep\n",
      "Completed the 242^th timestep\n",
      "Completed the 243^th timestep\n",
      "Completed the 244^th timestep\n",
      "Completed the 245^th timestep\n",
      "Completed the 246^th timestep\n",
      "Completed the 247^th timestep\n",
      "Completed the 248^th timestep\n",
      "Completed the 249^th timestep\n",
      "Completed the 250^th timestep\n",
      "Completed the 251^th timestep\n",
      "Completed the 252^th timestep\n",
      "Completed the 253^th timestep\n",
      "Completed the 254^th timestep\n",
      "Completed the 255^th timestep\n",
      "Completed the 256^th timestep\n",
      "Completed the 257^th timestep\n",
      "Completed the 258^th timestep\n",
      "Completed the 259^th timestep\n",
      "Completed the 260^th timestep\n",
      "Completed the 261^th timestep\n",
      "Completed the 262^th timestep\n",
      "Completed the 263^th timestep\n",
      "Completed the 264^th timestep\n",
      "Completed the 265^th timestep\n",
      "Completed the 266^th timestep\n",
      "Completed the 267^th timestep\n",
      "Completed the 268^th timestep\n",
      "Completed the 269^th timestep\n",
      "Completed the 270^th timestep\n",
      "Completed the 271^th timestep\n",
      "Completed the 272^th timestep\n",
      "Completed the 273^th timestep\n",
      "Completed the 274^th timestep\n",
      "Completed the 275^th timestep\n",
      "Completed the 276^th timestep\n",
      "Completed the 277^th timestep\n",
      "Completed the 278^th timestep\n",
      "Completed the 279^th timestep\n",
      "Completed the 280^th timestep\n",
      "Completed the 281^th timestep\n",
      "Completed the 282^th timestep\n",
      "Completed the 283^th timestep\n",
      "Completed the 284^th timestep\n",
      "Completed the 285^th timestep\n",
      "Completed the 286^th timestep\n",
      "Completed the 287^th timestep\n",
      "Completed the 288^th timestep\n",
      "Completed the 289^th timestep\n",
      "Completed the 290^th timestep\n",
      "Completed the 291^th timestep\n",
      "Completed the 292^th timestep\n",
      "Completed the 293^th timestep\n",
      "Completed the 294^th timestep\n",
      "Completed the 295^th timestep\n",
      "Completed the 296^th timestep\n",
      "Completed the 297^th timestep\n",
      "Completed the 298^th timestep\n",
      "Completed the 299^th timestep\n",
      "Completed the 300^th timestep\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, N+1):\n",
    "    \n",
    "    # Predictor step\n",
    "    n_list = compute_nlist(coords[0])\n",
    "    u = ut[:, :, :, n - 1]\n",
    "    \n",
    "    # Derivatives for predictor\n",
    "    ux_d = SPH(u[0], h, 0)\n",
    "    uy_d = SPH(u[1], h, 1)\n",
    "    ux_dd = SPH_d(ux_d, h)\n",
    "    uy_dd = SPH_d(uy_d, h)\n",
    "    \n",
    "    # Compute right-hand side for predictor\n",
    "    rhsx = torch.einsum('ji,ki,jlk->li', u[0], ux_d[0], c) + torch.einsum('ji,ki,jlk->li', u[1], ux_d[1], c) \\\n",
    "           - torch.einsum('jl,ji->li',d,(ux_dd[0] + ux_dd[1]))\n",
    "    rhsy = torch.einsum('ji,ki,jlk->li', u[0], uy_d[0], c) + torch.einsum('ji,ki,jlk->li', u[1], uy_d[1], c) \\\n",
    "           - torch.einsum('jl,ji->li',d,(uy_dd[0] + uy_dd[1]))\n",
    "    \n",
    "    # Predict new values of u (predictor estimate)\n",
    "    # u_pred_x = u[0] - rhsx * dt\n",
    "    # u_pred_y = u[1] - rhsy * dt\n",
    "    ut[0, :, :, n] = u[0] - rhsx.contiguous() * dt\n",
    "    ut[1, :, :, n] = u[1] - rhsy.contiguous() * dt\n",
    "    \n",
    "    # Boundary conditions\n",
    "    ut[0, :, torch.logical_or(x_mask,y_mask), n] = 0\n",
    "    ut[1, :, torch.logical_or(x_mask,y_mask), n] = 0\n",
    "\n",
    "    coords = coords +  (ut[:,:,:,n]).transpose(0,2).transpose(0,1) * dt\n",
    "    coordinates[:,:,:,n]=coords\n",
    "    print(f\"Completed the {n}^th timestep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('results_pce3_new.pkl', 'wb') as file:\n",
    "    pickle.dump(ut.numpy(), file)\n",
    "with open('coords_pce3_new.pkl', 'wb') as file:\n",
    "    pickle.dump(coordinates.numpy(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken 56.46244146426519 mins\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time taken {(t2-t1)/60} mins\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
